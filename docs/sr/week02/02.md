---
lang: sr
lang-ref: ch.02
title: 2. Nedelja
date: 3 February 2020
translation-date: 15 Dec 2020
translator: EmaPajic
---

<!--
## Lecture part A

We start by understanding what parametrised models are and then discuss what a loss function is. We then look at Gradient-based methods and how it's used in the backpropagation algorithm in a traditional neural network. We conclude this section by learning how to implement a neural network in PyTorch followed by a discussion on a more generalized form of backpropagation.
-->

## Lekcija, deo A

Započinjemo razumevanjem šta su parametarski modeli i diskutujemo šta je funkcija gubitka. Zatim razmatramo metode zasnovane na gradijentu i kako se koriste u algoritmu propagacije unazad u tradicionalnoj neuronskoj mreži. Zaključujemo ovu sekciju učenjem kako implementirati neuronsku mrežu u PyTorch-u i diskusijom o generalizovanijoj formi propagacije unazad.

<!--
## Lecture part B

We begin with a concrete example of backpropagation and discuss the dimensions of Jacobian matrices. We then look at various basic neural net modules and compute their gradients, followed by a brief discussion on softmax and logsoftmax. The other topic of discussion in this part is Practical Tricks for backpropagation.
-->

## Lekcija, deo B

Počicnjemo sa konkretnim primerom propagacije unazad i diskutujemo dimenzije Jakobijevih matrica. Zatim posmatramo razne bazične module neuronskih mreža i računamo njihove gradijente, uz kratku diskusiju o softmax i logsoftmax funkcijama. Druga tema ove lekcije je praktični trikovi propagacije unazad.

<!--
## Practicum

We give a brief introduction to supervised learning using artificial neural networks. We expound on the problem formulation and conventions of data used to train these networks. We also discuss how to train a neural network for multi class classification, and how to perform inference once the network is trained.
-->

## Praktikum

Prolazimo kroz kratki uvod u nadgledano učenje pomoću neuronskih mreža. Objašnjavamo problem formulacije i konvencije podataka korišćenih za obučavanje ovih mreža. Takođe diskutujemo kako obučiti neuronsku mrežu za klasifikaciju više klasa i kako vršiti predikciju nakon što je mreža obučena.
